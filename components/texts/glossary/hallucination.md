---
term_id: hallucination
title: "Hallucination"
related_terms: hallucination,hallucinations,hallucinate
---

A hallucination is when an [LLM] asserts false information like citing a source that does not exist. This is because the large language model is reproducing patterns from the data upon which they were trained. Hallucinations are considered one of the main problems with LLMs. 

