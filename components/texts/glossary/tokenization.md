---
term_id: tokenization
title: "Tokenization"
related_terms: tokenize,tokenizes,tokenizing
---

Tokenization is the act of splitting text into tokens. In language, tokenization is the breakdown of sentences into their parts which includes individual words and punctuation. 

It is used in OCR to make the text machine-readable.


